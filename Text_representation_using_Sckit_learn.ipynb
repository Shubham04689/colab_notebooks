{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubham04689/colab_notebooks/blob/main/Text_representation_using_Sckit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-9LguthAIGs"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- To understand several techniques in Text representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snBfzAFAkIFA"
      },
      "source": [
        "### Dataset\n",
        "   Here we will be using Movies_review data which contains 50000 reviews. The training data and testing are split evenly, 25k reviews under reviews_train and 25k under reviews_test.\n",
        "Under each file first 12500 reviews are positive and remaining 12500 are negative reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "def download_and_extract_dataset(url, extract_path='.'):\n",
        "  \"\"\"Downloads and extracts a dataset from a given URL.\n",
        "\n",
        "  Args:\n",
        "    url: The URL of the dataset.\n",
        "    extract_path: The path to extract the dataset to.\n",
        "  \"\"\"\n",
        "\n",
        "  # Download the dataset\n",
        "  response = requests.get(url, stream=True)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  # Save the dataset to a temporary file\n",
        "  with open('temp_dataset.tar.gz', 'wb') as f:\n",
        "    for chunk in response.iter_content(chunk_size=8192):\n",
        "      f.write(chunk)\n",
        "\n",
        "  # Extract the dataset\n",
        "  with tarfile.open('temp_dataset.tar.gz') as tar:\n",
        "    tar.extractall(extract_path)\n",
        "\n",
        "  # Remove the temporary file\n",
        "  os.remove('temp_dataset.tar.gz')\n",
        "\n",
        "# Example usage:\n",
        "dataset_url = 'https://cdn.talentsprint.com/aiml/movie_data.tar.gz'\n",
        "download_and_extract_dataset(dataset_url)"
      ],
      "metadata": {
        "id": "8dDBRoNtiKuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extarct data"
      ],
      "metadata": {
        "id": "A2Gdm0f0915i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYUL2aOrfPm0"
      },
      "source": [
        "### Importing required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsXb4LeA8qPA"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZv8zJIt8zy4"
      },
      "source": [
        "# Read each line and append to a list\n",
        "reviews_train = []\n",
        "\n",
        "for line in open(\"/content/movie_data/full_train.txt\", \"r\"):\n",
        "    reviews_train.append(line.strip()) # .strip() Return a copy of the string with leading and trailing whitespace removed\n",
        "\n",
        "reviews_test = []\n",
        "\n",
        "for line in open(\"/content/movie_data/full_test.txt\", \"r\"):\n",
        "    reviews_test.append(line.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBZqzPqR8_md"
      },
      "source": [
        "# Read the 20000th review from train file\n",
        "reviews_train[19999]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekBGXYC2_S-8"
      },
      "source": [
        "Replace_without_space = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")    # All these characters in text will be removed\n",
        "Replace_with_space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")            # All these characters in text will be replaced by space\n",
        "NO_SPACE = \"\"\n",
        "SPACE = \" \"\n",
        "\n",
        "def preprocess_reviews(reviews):\n",
        "    reviews = [Replace_without_space.sub(NO_SPACE, line.lower()) for line in reviews]\n",
        "    reviews = [Replace_with_space.sub(SPACE, line) for line in reviews]\n",
        "    return np.array(reviews)\n",
        "\n",
        "reviews_train_clean = preprocess_reviews(reviews_train)\n",
        "reviews_test_clean = preprocess_reviews(reviews_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O_3LXcs_of2"
      },
      "source": [
        "# Verify the 20000th review from train text file\n",
        "reviews_train_clean[19999]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCbUX25rDkD0"
      },
      "source": [
        "Give labels for the movie reviews, where first 12500 reviews are positive and remaining 12500 are negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVNZvV2oy7FD"
      },
      "source": [
        "target = np.array([1 if i < 12500 else 0 for i in range(25000)])  # Labeling positive reviews as 1 and negative reviews as 0\n",
        "print(target.shape, target[345], target[20000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9MnDcKliORP"
      },
      "source": [
        "### CountVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiz7XKrmQ9l3"
      },
      "source": [
        "Using N-grams get the consecutive words from the given text and get the feature vector using the countvectorizer for the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4ocUqUv_sLf"
      },
      "source": [
        "\"\"\"To get binary values (1 for present or 0 for absent) instead of counts of terms/tokens, give binary=True.\n",
        "N-Gram range basically lets you decide the length of the sequence of consecutive words in the given text. Suppose the n-gram range = (1, 3).\n",
        "Then it will pick the unigram(only single word), bigram (group of 2 consecutive words), and the trigram (group of 3 consecutive words).\"\"\"\n",
        "\n",
        "ngram_vectorizer = CountVectorizer(binary=False, ngram_range=(1, 2))\n",
        "ngram_vectorizer.fit(reviews_train_clean)                         # Tokenize and build vocab\n",
        "train_vec = ngram_vectorizer.transform(reviews_train_clean)       # To get feature vector for train data\n",
        "test_vec = ngram_vectorizer.transform(reviews_test_clean)         # To get feature vector for test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeEuYxynkBtn"
      },
      "source": [
        "#### Split the review_train data into train and test sets\n",
        "\n",
        "Hint: Refer to[Train-Test split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7kCsHWMkA-M"
      },
      "source": [
        "# Split the train and test sets\n",
        "X_train,X_test, y_train,y_test = train_test_split(train_vec, target, test_size = 0.25,random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmJ2nArFtiBB"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpfOG53zlz5S"
      },
      "source": [
        "#### Apply the Decision Tree Classifier for the splitted review_train data\n",
        "Note: Below code cell take some time to compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm7QJlvEDMSk"
      },
      "source": [
        "# Create an object for the DecisionTreeClassifier\n",
        "decisiontree = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model and get the predictions\n",
        "decisiontree.fit(X_train,y_train)\n",
        "\n",
        "# Predict the model\n",
        "predict = decisiontree.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_score(y_test, predict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDfY6Q9ODNNn"
      },
      "source": [
        "# Use the trained model to get the predictions on the review_test data\n",
        "predict = decisiontree.predict(test_vec)\n",
        "accuracy_score(target, predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQr_afHnUcTL"
      },
      "source": [
        "### TF IDF\n",
        " tf-idf aims to represent the number of times a given word appears in a document (a movie review in our case) relative to the number of documents in the corpus that the word appears in — where, words that appear in many documents have a value closer to zero and words that appear in less documents have values closer to 1.\n",
        "\n",
        "We have seen how to get the consecutive words using n-grams, similarly you can try without using n-grams\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x75doewBlY-"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(reviews_train_clean)\n",
        "X_train_tfidf = tfidf_vectorizer.transform(reviews_train_clean)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(reviews_test_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXQcZ_plKMDh"
      },
      "source": [
        "#### Split the review_train data into train and test sets\n",
        "\n",
        "Hint: Refer to [Train-Test split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgjZaKTUKMDm"
      },
      "source": [
        "# Split the train and test sets\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X_train_tfidf,target,test_size=0.25, random_state= 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZwW_AFZxlXt"
      },
      "source": [
        "\n",
        "#### Apply the Decision Tree Classifier\n",
        "Note: Below code cell take some time to complie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1vkXQEExlXx"
      },
      "source": [
        "# Create an object of DecisionTreeClassifier\n",
        "decisiontree = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model and get the predictions\n",
        "decisiontree.fit(X1_train,y1_train)\n",
        "\n",
        "# Predict the model\n",
        "predict = decisiontree.predict(X1_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_score(y1_test, predict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYwelCgQxlYK"
      },
      "source": [
        "# Use the trained model to get the predictions on the review_test data\n",
        "predict = decisiontree.predict(X_test_tfidf)\n",
        "accuracy_score(target, predict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}